# 语音基座和多模态大模型

|基础论文|

|相关论文|

|数据集|

## 基础论文

#### 2024

- 复旦大学		 WavLLM: Towards Robust and Adaptive Speech Large Language Model [Paper](https://arxiv.org/abs/2404.00656) [ProjectPage](https://aka.ms/wavllm)
- 香港中文大学	 Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models [Paper](https://arxiv.org/abs/2403.18814) [ProjectPage](https://github.com/dvlab-research/MGM)
- 复旦大学		 AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling [Paper](https://arxiv.org/abs/2402.12226) [PojectPage](https://junzhan2000.github.io/AnyGPT.github.io/)
- 香港理工大学	 Dual Parameter-Efficient Fine-Tuning for Speaker Representation Via Speaker Prompt Tuning and Adapters [Paper](https://ieeexplore.ieee.org/document/10447795)
- 卡耐基梅隆大学     Domain Adaptation for Contrastive Audio-Language Models [Paper](https://arxiv.org/abs/2402.09585)


#### 2023


- 复旦大学		SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models [Paper](https://arxiv.org/abs/2308.16692) [ProjectPage](https://0nutation.github.io/SpeechTokenizer.github.io/)
- 复旦大学		SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities [Paper](https://arxiv.org/abs/2305.11000) [ProjectPage](SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities)
- IEEE			Parameter-Efficient Transfer Learning of Audio Spectrogram Transformers [Paper](https://arxiv.org/abs/2312.03694)
- 国立台湾大学	Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech [Paper](https://arxiv.org/abs/2309.09510)
- 国立台湾大学	SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts [Paper](https://arxiv.org/abs/2306.02207)
- 阿里巴巴		Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models [Paper](https://arxiv.org/abs/2311.07919) [ProjectPage](https://github.com/QwenLM/Qwen-Audio)
- 美国微软		On decoder-only architecture for speech-to-text and large language model integration [Paper](https://arxiv.org/abs/2307.03917)
- 国立台湾大学	SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks [Paper](https://arxiv.org/abs/2303.00733)
- 微软			Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers [Paper](https://arxiv.org/abs/2301.02111) [ProjectPage](https://github.com/microsoft/unilm)
- 昆士兰科技大学    Sparks of Large Audio Models: A Survey and Outlook [Paper](https://arxiv.org/abs/2308.12792) [ProjectPage](https://github.com/EmulationAI/awesome-large-audio-models)


#### 2022

- 国立台湾大学	SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks [Paper]()
- 布尔诺科技大学    Parameter-efficient transfer learning of pre-trained Transformer models for speaker verification using adapters  [Paper](https://arxiv.org/abs/2210.16032)

## 相关论文

#### 2024

- 中国科学院	    BLSP-KD: Bootstrapping Language-Speech Pre-training via Knowledge Distillation [Paper](https://arxiv.org/abs/2405.19041)
- 腾讯AI实验室	SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation [Paper](https://arxiv.org/abs/2404.14396) [ProjectPage](https://github.com/AILab-CVC/SEED-X)
- 清华大学		RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness [Paper](https://arxiv.org/abs/2405.17220) [ProjectPage](https://github.com/RLHF-V/RLAIF-V)
- 中国人民大学	TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning [Paper](https://arxiv.org/abs/2404.16635) [ProjectPage]()
- 腾讯AI实验室	MM-LLMs: Recent Advances in MultiModal Large Language Models [Paper](https://arxiv.org/abs/2401.13601) [ProjectPage](https://mm-llms.github.io/) 
- 字节跳动		MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering [Paper](https://arxiv.org/abs/2405.11985) [ProjectPage](https://bytedance.github.io/MTVQA/)
- 浙江大学		Auto-Encoding Morph-Tokens for Multimodal LLM [Paper](https://arxiv.org/abs/2405.01926) [ProjectPage](https://github.com/DCDmllm/MorphTokens)
- 南洋理工大学	F-LMM: Grounding Frozen Large Multimodal Models [Paper](https://arxiv.org/abs/2406.05821) [ProjectPage](https://github.com/wusize/F-LMM)
- 新加坡国立大学    Towards Semantic Equivalence of Tokenizationin Multimodal LLM [Paper](https://arxiv.org/pdf/2406.05127) [ProjectPage](https://chocowu.github.io/SeTok-web/)
- 滑铁卢大学	    MANTIS: Interleaved Multi-Image Instruction Tuning [Paper](https://arxiv.org/abs/2405.01483) [ProjectPage](https://tiger-ai-lab.github.io/Mantis/)
- 哈尔滨工业大学    Recognizing Everything from All Modalities at Once Grounded Multimodal Universal Information Extraction [Paper](https://arxiv.org/abs/2406.03701)
- 香港中文大学	UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner [Paper](https://arxiv.org/abs/2406.10056) [ProjectPage](https://github.com/yangdongchao/LLM-Codec)
- 微软			NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models [Paper](https://arxiv.org/abs/2403.03100) [ProjectPage](https://speechresearch.github.io/)
- 腾讯优图Lab	 Efficient Multimodal Large Language Models: A Survey [Paper](https://arxiv.org/pdf/2405.10739) 


## 数据集

- [LibriSpeech](http://www.openslr.org/12/)
- [Fisher](https://catalog.ldc.upenn.edu/LDC2004T19)
- [Switchboard](https://catalog.ldc.upenn.edu/LDC97S62)
- [CoVoST2](https://www.statmt.org/wmt16/multimodal-task.html)

- [MuST-C](https://www.statmt.org/wmt17/multimodal-task.html)

- [VoxCeleb](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)

- [LAION-GPT-4V](https://huggingface.co/datasets/laion/gpt4v-dataset)
- [Gigaspeech](https://github.com/SpeechColab/GigaSpeech)

- [CommonVoice](https://commonvoice.mozilla.org/en/datasets)

- [Laion coco](https://laion.ai/blog/laion-coco/)

- [VCTK](https://datashare.ed.ac.uk/handle/10283/3443)

- [ESC-50](https://github.com/karolpiczak/ESC-50)
- [UrbanSound8K](https://urbansounddataset.weebly.com/)

- [DCASE2017 Task4](https://dcase.community/challenge2017/task-acoustic-scene-classification)
- [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D)

- [RAVDESS](https://zenodo.org/records/1188976)

- [LibriLight](http://www.openslr.org/95/)

- [TIMIT](https://catalog.ldc.upenn.edu/LDC93S1)

- [AISHELL-3](http://www.openslr.org/93/)

- [MTVQA](https://huggingface.co/datasets/ByteDance/MTVQA)
- [MSCOCO2017](https://cocodataset.org/#home)